{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPPwXaIJ7UQZP/ysTdv5T54",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/nguyenhaphan1/ID-recognition-with-CRNN-/blob/main/DigitRecognitionWithCRNN.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UQfvQB1icQMY",
        "outputId": "8cee0a77-4682-4acf-fd68-347c3aa7c299"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Clone pre-implemented CRNN model from git\n",
        "!git clone https://github.com/FLming/CRNN.tf2"
      ],
      "metadata": {
        "id": "0NIh6JPucWFN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -r requirements.txt"
      ],
      "metadata": {
        "id": "m8SCP4gmeKX0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import tensorflow as tf\n",
        "import cv2\n",
        "from tensorflow import keras\n",
        "from google.colab.patches import cv2_imshow\n",
        "import numpy as np"
      ],
      "metadata": {
        "id": "eamADSV2dKid"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#define loss and accuracy class for loading the model\n",
        "class CTCLoss(keras.losses.Loss):\n",
        "    \"\"\"A class that wraps the function of tf.nn.ctc_loss.\n",
        "\n",
        "    Attributes:\n",
        "        logits_time_major: If False (default) , shape is [batch, time, logits],\n",
        "            If True, logits is shaped [time, batch, logits].\n",
        "        blank_index: Set the class index to use for the blank label. default is\n",
        "            -1 (num_classes - 1).\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(\n",
        "        self, logits_time_major=False, blank_index=-1, reduction=keras.losses.Reduction.AUTO, name=\"ctc_loss\"\n",
        "    ):\n",
        "        super().__init__(name=name, reduction=reduction)\n",
        "        self.logits_time_major = logits_time_major\n",
        "        self.blank_index = blank_index\n",
        "\n",
        "    def call(self, y_true, y_pred):\n",
        "        \"\"\"Computes CTC (Connectionist Temporal Classification) loss. Works on\n",
        "        CPU, because y_true is a SparseTensor.\n",
        "        \"\"\"\n",
        "        y_true = tf.cast(y_true, tf.int32)\n",
        "        y_pred_shape = tf.shape(y_pred)\n",
        "        logit_length = tf.fill([y_pred_shape[0]], y_pred_shape[1])\n",
        "        loss = tf.nn.ctc_loss(\n",
        "            labels=y_true,\n",
        "            logits=y_pred,\n",
        "            label_length=None,\n",
        "            logit_length=logit_length,\n",
        "            logits_time_major=self.logits_time_major,\n",
        "            blank_index=self.blank_index,\n",
        "        )\n",
        "        return tf.math.reduce_mean(loss)\n",
        "\n",
        "class SequenceAccuracy(keras.metrics.Metric):\n",
        "    def __init__(self, name=\"sequence_accuracy\", **kwargs):\n",
        "        super().__init__(name=name, **kwargs)\n",
        "        self.total = self.add_weight(name=\"total\", initializer=\"zeros\")\n",
        "        self.count = self.add_weight(name=\"count\", initializer=\"zeros\")\n",
        "\n",
        "    def update_state(self, y_true, y_pred, sample_weight=None):\n",
        "        def sparse2dense(tensor, shape):\n",
        "            tensor = tf.sparse.reset_shape(tensor, shape)\n",
        "            tensor = tf.sparse.to_dense(tensor, default_value=-1)\n",
        "            tensor = tf.cast(tensor, tf.float32)\n",
        "            return tensor\n",
        "\n",
        "        y_true_shape = tf.shape(y_true)\n",
        "        batch_size = y_true_shape[0]\n",
        "        y_pred_shape = tf.shape(y_pred)\n",
        "        max_width = tf.math.maximum(y_true_shape[1], y_pred_shape[1])\n",
        "        logit_length = tf.fill([batch_size], y_pred_shape[1])\n",
        "        decoded, _ = tf.nn.ctc_greedy_decoder(\n",
        "            inputs=tf.transpose(y_pred, perm=[1, 0, 2]),\n",
        "            sequence_length=logit_length,\n",
        "        )\n",
        "        y_true = sparse2dense(y_true, [batch_size, max_width])\n",
        "        y_pred = sparse2dense(decoded[0], [batch_size, max_width])\n",
        "        num_errors = tf.math.reduce_any(\n",
        "            tf.math.not_equal(y_true, y_pred), axis=1\n",
        "        )\n",
        "        num_errors = tf.cast(num_errors, tf.float32)\n",
        "        num_errors = tf.math.reduce_sum(num_errors)\n",
        "        batch_size = tf.cast(batch_size, tf.float32)\n",
        "        self.total.assign_add(batch_size)\n",
        "        self.count.assign_add(batch_size - num_errors)\n",
        "\n",
        "    def result(self):\n",
        "        return self.count / self.total\n",
        "\n",
        "    def reset_states(self):\n",
        "        self.count.assign(0)\n",
        "        self.total.assign(0)\n",
        "\n",
        "\n",
        "class EditDistance(keras.metrics.Metric):\n",
        "    def __init__(self, name=\"edit_distance\", **kwargs):\n",
        "        super().__init__(name=name, **kwargs)\n",
        "        self.total = self.add_weight(name=\"total\", initializer=\"zeros\")\n",
        "        self.sum_distance = self.add_weight(\n",
        "            name=\"sum_distance\", initializer=\"zeros\"\n",
        "        )\n",
        "\n",
        "    def update_state(self, y_true, y_pred, sample_weight=None):\n",
        "        y_pred_shape = tf.shape(y_pred)\n",
        "        batch_size = y_pred_shape[0]\n",
        "        logit_length = tf.fill([batch_size], y_pred_shape[1])\n",
        "        decoded, _ = tf.nn.ctc_greedy_decoder(\n",
        "            inputs=tf.transpose(y_pred, perm=[1, 0, 2]),\n",
        "            sequence_length=logit_length,\n",
        "        )\n",
        "        sum_distance = tf.math.reduce_sum(tf.edit_distance(decoded[0], y_true))\n",
        "        batch_size = tf.cast(batch_size, tf.float32)\n",
        "        self.sum_distance.assign_add(sum_distance)\n",
        "        self.total.assign_add(batch_size)\n",
        "\n",
        "    def result(self):\n",
        "        return self.sum_distance / self.total\n",
        "\n",
        "    def reset_states(self):\n",
        "        self.sum_distance.assign(0)\n",
        "        self.total.assign(0)\n"
      ],
      "metadata": {
        "id": "Av7Yjo0yc1sd"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "BTg_ww2fdIJl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**1. Train the model with custom dataset**"
      ],
      "metadata": {
        "id": "w610_CfZdSEt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!python CRNN.tf2/crnn/train.py --config mjsynth.yml --save_dir 'runs/train'"
      ],
      "metadata": {
        "id": "F74Pep3adV-q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**2. Load the trained model from google drive**"
      ],
      "metadata": {
        "id": "CrHyubwTfdkj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Load trained model\n",
        "model = keras.models.load_model(\n",
        "    'model.keras',\n",
        "    custom_objects={'CTCLoss': CTCLoss, 'SequenceAccuracy': SequenceAccuracy}\n",
        ")"
      ],
      "metadata": {
        "id": "UVWDR6Bedld-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**3. Predict with trained model**"
      ],
      "metadata": {
        "id": "hYvyuxYPfkPJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def clean_string(string):\n",
        "  cleaned_string = ''\n",
        "  new_check = False\n",
        "  i = 0\n",
        "  while i < len(string):\n",
        "    # print(f\"i: {i}\")\n",
        "    if i == len(string) - 1 and string[i] != string[i-1] and string[i] != '|':\n",
        "      cleaned_string += string[i]\n",
        "      break\n",
        "    elif i == len(string) - 1 and string[i] == string[i-1] and string:\n",
        "      if string[i] != cleaned_string[-1] and string[i] != '|':\n",
        "        cleaned_string += string[i]\n",
        "        break\n",
        "      elif string[i] != '|' and new_check:\n",
        "        cleaned_string += string[i]\n",
        "        break\n",
        "      else:\n",
        "        break\n",
        "\n",
        "    if string[i] == '|':\n",
        "      i += 1\n",
        "      new_check = True\n",
        "      continue\n",
        "\n",
        "    if string[i] != string[i+1]:\n",
        "      # print(string[i])\n",
        "      cleaned_string += string[i]\n",
        "      i += 1\n",
        "      new_check = False\n",
        "      continue\n",
        "\n",
        "    if string[i] == string[i+1]:\n",
        "      i += 1\n",
        "      continue\n",
        "  return cleaned_string\n",
        "\n",
        "def translate_predict(pred):\n",
        "  string = ''\n",
        "  cleaned_string = ''\n",
        "  # print(pred[0])\n",
        "  for result in pred[0]:\n",
        "    digit = str(np.argmax(result))\n",
        "    if digit == '10':\n",
        "      string += '|'\n",
        "      continue\n",
        "    string += str(np.argmax(result))\n",
        "  print(string)\n",
        "  cleaned_string = clean_string(string)\n",
        "  return cleaned_string\n",
        "\n",
        "def predict(image_path):\n",
        "  image = cv2.imread(image_path)\n",
        "  image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
        "  cv2_imshow(image)\n",
        "  height_ratio = image.shape[0]/32\n",
        "  image = cv2.resize(image, (int(image.shape[1]/ratio), 32))\n",
        "  image = image.reshape(1, image.shape[0], image.shape[1], 3)\n",
        "  image = image / 255.0\n",
        "  pred = model.predict(image)\n",
        "  return pred"
      ],
      "metadata": {
        "id": "tf-ewi3Udq63"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "image_path = cv2.imread('/content/drive/MyDrive/CCCD/data_for_prediction/new_test_data_ids5/811.jpg')\n",
        "\n",
        "pred = predict(image_path)\n",
        "print(f\"Model prediction: {translate_predict(pred)}\")"
      ],
      "metadata": {
        "id": "M99dn4LweyVM"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}